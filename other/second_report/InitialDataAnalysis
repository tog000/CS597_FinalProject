1. Copy of the dataset (load it on the onyx server and provide the location) (20 points)
We are using the crime dataset from the Chicago open data project. The link can be found here 

https://data.cityofchicago.org/Public-Safety/Crimes-2001-to-present/ijzp-q8t2

On onyx, the dataset can be found at 

/home/students/dtanner/bigdata/chicagocrimes.csv


2. Analysis of the data (For example, if you were using white house dataset from 2011-2013 to predict the number of times person B will visit person A in 2014, provide an explanation of why do you think that data would be able to help answer that question?) (20 points)
Since we are planning on developing some sort of predictive analyzer that, based on time, location and other factors, what types of crimes will be committed in Chicago this data set will help us by, after some analysis, providing a labeled training set for our model. There are many useful patterns that we can draw from this dataset, the first of which we recognized is that if we can determine the frequencies of certain crimes by time and location, we can tune the weights to provide a good accuracy when predicting future crimes.

3. Data cleansing issues that you identified and how did you address them? (10 points)
For the most part, the data was actually quite clean so there were very minimal issues; some typical unstructured data problems but nothing that was irrecoverable. At first, we thought one of the more critical pieces of data we would need is the latitude and longitude coordinates and that is true in many cases but in order to do analysis at a higher level, it seemed better to have a less granular judge of proximity. Through some research we discovered that Chicago is broken up in to approximately 75 "community area's"; unfortunately, the amount community area's specified in the dataset was quite small so our initial analysis largely consisted of figuring out which crimes occured in which community area. Other issues we had were trivial: casing issues which were solved by normalizing all casing during preprocessing and our data was in csv form but there were some comma's within a single field in the set so there was some extra reg-ex fanciness to overcome it. 
